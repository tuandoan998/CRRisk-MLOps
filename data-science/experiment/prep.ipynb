{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import warnings\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from dateutil.rrule import rrule, MONTHLY\n",
        "import mlflow\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1695115870406
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_BUG_RATE = 0.2\n",
        "RANDOM_STATE = 101\n",
        "FEATURE_COLS = [\n",
        "    \"Total Estimated Efforts\",\n",
        "    \"Deployment Month Error Rate\",\n",
        "    \"Duration\",\n",
        "    # \"CTASK Number\",\n",
        "    \"Module error rate\",\n",
        "    \"Related modules\",\n",
        "    \"Related members\",\n",
        "    \"Weighted Average Exp.\",\n",
        "    # \"Deviation Exp.\",\n",
        "    # \"AVG Dev Quality Rate by Exp. Year\",\n",
        "    # \"AVG Dev Quality Rate\",\n",
        "    # \"Weighted Average Module Ticket Count\",\n",
        "    # \"Weighted Average Ticket Count\",\n",
        "    \"UI error rate\",\n",
        "]\n",
        "LABEL_COL = \"is_bug_inc\""
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695115870750
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataframes(data_path, train_test_ratio):\n",
        "    # Read CR ticket\n",
        "    usecols = ['Ticket', 'Deployment Month',\n",
        "           'Total Estimated Efforts', 'CTASK Number',\n",
        "           'Duration', 'Tester Tested', 'Bugs', 'Incident count',\n",
        "           'AVG Dev Quality Rate', 'AVG Dev Quality Rate by Exp. Year']\n",
        "    df = pd.read_excel(data_path, sheet_name='CR-Data', usecols=usecols)\n",
        "\n",
        "\n",
        "    df_new = pd.read_excel(data_path, sheet_name='TestingData')\n",
        "    df_new = df_new[df_new['CR Stage']=='Closed']\n",
        "    df_new = df_new[usecols]\n",
        "\n",
        "    df = pd.concat([df, df_new])\n",
        "    df = df.dropna()\n",
        "    df['Deployment Month'] = pd.to_datetime(df['Deployment Month'])\n",
        "    df = df[df['Deployment Month']>datetime.datetime(2019, 1, 1)]\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # read task data\n",
        "    df_task = pd.read_excel(data_path, sheet_name='Ctask')\n",
        "    df_task_new = pd.read_excel(data_path, sheet_name='TestingDataCtask',\n",
        "                                usecols=set(df_task.columns)-{'Members'})\n",
        "    df_task = pd.concat([df_task, df_task_new])\n",
        "    df_task = df_task[df_task['CR'].isin(df['Ticket'].unique())]\n",
        "    df_task = df_task.reset_index(drop=True)\n",
        "    ### fill estimated effort with actual and vice versa\n",
        "    df_task['Est. Effort'] = df_task.apply(lambda x: x['Est. Effort'] if x['Est. Effort']!=0 else\n",
        "                                                    x['Act. Effort'], axis=1)\n",
        "\n",
        "    df_task['Act. Effort'] = df_task.apply(lambda x: x['Act. Effort'] if x['Act. Effort']!=0 else\n",
        "                                                    x['Est. Effort'], axis=1)\n",
        "\n",
        "    ### fill 1 to the rest\n",
        "    df_task['Act. Effort'] = df_task.apply(lambda x: 1 if x['Act. Effort']==0 and x['Est. Effort']==0 else\n",
        "                                                        x['Act. Effort'], axis=1)\n",
        "\n",
        "    df_task['Est. Effort'] = df_task.apply(lambda x: 1 if x['Act. Effort']==1 and x['Est. Effort']==0 else\n",
        "                                                        x['Est. Effort'], axis=1)\n",
        "\n",
        "    # read member data\n",
        "    df_member = pd.read_excel(data_path, sheet_name='setup', usecols=['Member', 'Emp Code', 'Start working'])\n",
        "    df[\"ttl_bug_inc\"] = df[\"Bugs\"] + df[\"Incident count\"]\n",
        "    df['is_bug_inc'] = df['ttl_bug_inc'].apply(lambda x: 1 if x>0 else 0)\n",
        "\n",
        "    BASE_THRESHOLD = df['Deployment Month'].sort_values().quantile(train_test_ratio)\n",
        "    base_cr_id = df[df['Deployment Month']<BASE_THRESHOLD]['Ticket'].values\n",
        "    df_task_base = df_task[df_task['CR'].isin(base_cr_id)].reset_index(drop=True)\n",
        "\n",
        "    df_task_base[\"ttl_bug_inc\"] = df_task_base[\"Bugs\"] + df_task_base[\"Incident count\"]\n",
        "    df_task_base['is_bug_inc'] = df_task_base['ttl_bug_inc'].apply(lambda x: 1 if x>0 else 0)\n",
        "\n",
        "    return df, df_task, df_task_base, df_member, base_cr_id\n",
        "\n",
        "\n",
        "# Ticket features\n",
        "def add_ticket_features(df, df_task, df_task_base):\n",
        "    related_modules_dict = df_task.groupby('CR')['Module code'].nunique().to_dict()\n",
        "    df['Related modules'] = df['Ticket'].apply(lambda x: related_modules_dict[x])\n",
        "\n",
        "    ### task level module error rate\n",
        "    module_error_rate = (df_task_base.drop_duplicates(['CR', 'Module code']).groupby('Module code')['ttl_bug_inc'].sum() /\n",
        "                        df_task_base.drop_duplicates(['CR', 'Module code']).groupby('Module code')['ttl_bug_inc'].count()).to_dict()\n",
        "    ### CR level module error rate\n",
        "    cr_error_rate_base = {}\n",
        "    for cr_id, cr_group in df_task.groupby('CR'):\n",
        "        module_effort = cr_group.groupby('Module code')['Est. Effort'].sum()\n",
        "        module_err_rate = np.average(a=[module_error_rate.get(item, DEFAULT_BUG_RATE) for item in module_effort.index],\n",
        "                                    weights=module_effort.values)\n",
        "        cr_error_rate_base[cr_id] = module_err_rate\n",
        "\n",
        "    df_task[\"ttl_bug_inc\"] = df_task[\"Bugs\"] + df_task[\"Incident count\"]\n",
        "    df_task['is_bug_inc'] = df_task['ttl_bug_inc'].apply(lambda x: 1 if x>0 else 0)\n",
        "    df_task = df_task.sort_values('Date Closed').reset_index(drop=True)\n",
        "\n",
        "    ### task level module error rate\n",
        "    df_cr_module_bug = df_task.sort_values('Date Closed')[['CR', 'Module code', 'ttl_bug_inc']].drop_duplicates()\n",
        "    cr_module_error_rate = {}\n",
        "    module_error_rate = {}\n",
        "    for module in df_task['Module code'].unique():\n",
        "        df_cr_module_error = df_cr_module_bug[df_cr_module_bug['Module code']==module].reset_index(drop=True)\n",
        "        cumsum_error = df_cr_module_error['ttl_bug_inc'].shift().cumsum().fillna(0)\n",
        "        cumsum_error_rate = cumsum_error / (cumsum_error.index+1)\n",
        "        df_cr_module_error['cumsum_error_rate'] = cumsum_error_rate\n",
        "        cr_module_error_rate.update(df_cr_module_error.set_index(['CR', 'Module code'])['cumsum_error_rate'].to_dict())\n",
        "        module_error_rate.update(df_cr_module_error.set_index(['Module code'])['cumsum_error_rate'].to_dict())\n",
        "\n",
        "    ### CR level module error rate\n",
        "    cr_error_rate_uptodate = {}\n",
        "    for cr_id, cr_group in df_task.groupby('CR'):\n",
        "        module_effort = cr_group.groupby('Module code')['Est. Effort'].sum()\n",
        "        module_err_rate = np.average(a=[cr_module_error_rate.get((cr_id, module), DEFAULT_BUG_RATE) for module in module_effort.index],\n",
        "                                    weights=module_effort.values)\n",
        "        cr_error_rate_uptodate[cr_id] = module_err_rate\n",
        "\n",
        "    df['Module error rate'] = df['Ticket'].apply(lambda x: cr_error_rate_base[x] if x in df_task_base['CR'].unique() else\n",
        "                                                        cr_error_rate_uptodate[x])\n",
        "\n",
        "    return df, df_task, module_error_rate, cr_error_rate_uptodate\n",
        "\n",
        "# Dev Features\n",
        "def add_dev_features(df, df_member, df_task):\n",
        "    related_members_dict = df_task.groupby('CR')['Dev Emp Cd'].nunique().to_dict()\n",
        "    df['Related members'] = df['Ticket'].apply(lambda x: related_members_dict[x])\n",
        "\n",
        "    # Dev exp by months\n",
        "    member_startdate_dict = df_member.set_index('Emp Code')['Start working'].dropna().to_dict()\n",
        "    def get_dev_clv_exp(task):\n",
        "        try:\n",
        "            start_date = member_startdate_dict[task['Dev Emp Cd']]\n",
        "            clv_exp = (len(list(rrule(MONTHLY, dtstart=start_date, until=task['Start Developing'])))-1)/12\n",
        "        except:\n",
        "            clv_exp = 0\n",
        "        return clv_exp\n",
        "    df_task['CLV exp'] = df_task.apply(get_dev_clv_exp, axis=1)\n",
        "    ### Weighted Average exp\n",
        "    cr_weighted_exp_year = {}\n",
        "    for cr_id, cr_group in df_task.groupby('CR'):\n",
        "        emp_weighted_exp_year = np.average(a=cr_group.groupby('Dev Emp Cd')['CLV exp'].mean().sort_index().values,\n",
        "                                        weights=cr_group.groupby('Dev Emp Cd')['Est. Effort'].sum().sort_index().values)\n",
        "        cr_weighted_exp_year[cr_id] = emp_weighted_exp_year\n",
        "    df['Weighted Average Exp.'] = df['Ticket'].apply(lambda x: cr_weighted_exp_year[x])\n",
        "\n",
        "    # Dev exp by number of tickets\n",
        "    cr_emp_exp_ticket = {}\n",
        "    ## For each employee\n",
        "    for emp_code in df_task['Dev Emp Cd'].unique():\n",
        "        df_tmp_emp_exp_ticket = df_task[df_task['Dev Emp Cd']==emp_code][['CR', 'Dev Emp Cd']].drop_duplicates().reset_index(drop=True)\n",
        "        ## Use cumsum to count tickets in the past of this employee\n",
        "        df_tmp_emp_exp_ticket['Num Ticket'] = 1\n",
        "        df_tmp_emp_exp_ticket['Num Ticket'] = df_tmp_emp_exp_ticket['Num Ticket'].shift().cumsum().fillna(0)\n",
        "        cr_emp_exp_ticket.update(df_tmp_emp_exp_ticket.set_index(['CR', 'Dev Emp Cd'])['Num Ticket'].to_dict())\n",
        "    ### Weighted Experience by ticket count\n",
        "    cr_weighted_exp_ticket = {}\n",
        "    for cr_id, cr_group in df_task.groupby('CR'):\n",
        "        dev_effort = cr_group.groupby('Dev Emp Cd')['Est. Effort'].sum()\n",
        "        emp_exp_ticket = np.average(a=[cr_emp_exp_ticket.get((cr_id, emp_code), DEFAULT_BUG_RATE) for emp_code in dev_effort.index],\n",
        "                                    weights=dev_effort.values)\n",
        "        cr_weighted_exp_ticket[cr_id] = emp_exp_ticket\n",
        "    ### Experience by ticket count\n",
        "    cr_exp_ticket = {}\n",
        "    for cr_id, cr_group in df_task.groupby('CR'):\n",
        "        dev_effort = cr_group.groupby('Dev Emp Cd')['Est. Effort'].sum()\n",
        "        emp_exp_ticket = np.mean([cr_emp_exp_ticket.get((cr_id, emp_code), DEFAULT_BUG_RATE) for emp_code in dev_effort.index])\n",
        "        cr_exp_ticket[cr_id] = emp_exp_ticket\n",
        "    df['Weighted Average Ticket Count'] = df['Ticket'].apply(lambda x: cr_weighted_exp_ticket[x])\n",
        "    df['Average Ticket Count'] = df['Ticket'].apply(lambda x: cr_exp_ticket[x])\n",
        "\n",
        "    # Dev exp by number of tickets of same module\n",
        "    cr_emp_module_exp_ticket = {}\n",
        "    for i, row in df_task[['Dev Emp Cd', 'Module code']].drop_duplicates().iterrows():\n",
        "        emp_code = row['Dev Emp Cd']\n",
        "        module_code = row['Module code']\n",
        "        df_tmp_emp_module_exp_ticket = df_task[(df_task['Dev Emp Cd']==emp_code)&\n",
        "                                            (df_task['Module code']==module_code)][['CR', 'Dev Emp Cd', 'Module code']].drop_duplicates().reset_index(drop=True)\n",
        "        df_tmp_emp_module_exp_ticket['Num Ticket'] = 1\n",
        "        df_tmp_emp_module_exp_ticket['Num Ticket'] = df_tmp_emp_module_exp_ticket['Num Ticket'].shift().cumsum().fillna(0)\n",
        "        cr_emp_module_exp_ticket.update(df_tmp_emp_module_exp_ticket.set_index(['CR', 'Dev Emp Cd', 'Module code'])['Num Ticket'].to_dict())\n",
        "    ### Weighted Experience by ticket of same module count\n",
        "    cr_weighted_module_exp_ticket = {}\n",
        "    for cr_id, cr_group in df_task.groupby('CR'):\n",
        "        module_effort = cr_group.groupby(['Dev Emp Cd', 'Module code'])['Est. Effort'].sum()\n",
        "        emp_module_exp_ticket = np.average(a=[cr_emp_module_exp_ticket.get((cr_id, emp_code, module_code), DEFAULT_BUG_RATE)\n",
        "                                            for emp_code, module_code in module_effort.index],\n",
        "                                        weights=module_effort.values)\n",
        "        cr_weighted_module_exp_ticket[cr_id] = emp_module_exp_ticket\n",
        "    df['Weighted Average Module Ticket Count'] = df['Ticket'].apply(lambda x: cr_weighted_module_exp_ticket[x])\n",
        "    dict_module_ticket_count_error = df.groupby(pd.cut(df['Weighted Average Module Ticket Count'], range(0, 71, 5)))['ttl_bug_inc'].sum().to_dict()\n",
        "    def get_module_ticket_count_error(val):\n",
        "        for k, v in dict_module_ticket_count_error.items():\n",
        "            if val in k:\n",
        "                return v\n",
        "        return 0\n",
        "    df['Module Ticket Count Error rate'] = df['Weighted Average Module Ticket Count'].apply(get_module_ticket_count_error)\n",
        "\n",
        "    return df, df_task\n",
        "\n",
        "\n",
        "# Other features\n",
        "def add_other_features(df, base_cr_id):\n",
        "    df_base = df[df['Ticket'].isin(base_cr_id)].reset_index(drop=True)\n",
        "    dict_deployment_month_error_rate = (df_base.groupby(df_base['Deployment Month'].dt.month)['is_bug_inc'].sum() /\n",
        "                                        df_base.groupby(df_base['Deployment Month'].dt.month)['Ticket'].count()).to_dict()\n",
        "    df['Deployment Month Error Rate'] = df['Deployment Month'].apply(lambda x: dict_deployment_month_error_rate[x.month])\n",
        "    return df\n",
        "\n",
        "\n",
        "# UI features\n",
        "def add_ui_features(df, df_task):\n",
        "    df_task['UI_Join'] = df_task[['Criterion 2', 'Criterion 3','Criterion 4']].fillna('').apply(lambda x: '/'.join(x), axis=1)\n",
        "\n",
        "    df_cr_ui_bug = df_task.sort_values('Date Closed')[['CR', 'UI_Join', 'ttl_bug_inc']].drop_duplicates()\n",
        "    cr_ui_error_rate = {}\n",
        "    for ui in df_task['UI_Join'].unique():\n",
        "        df_cr_ui_error = df_cr_ui_bug[df_cr_ui_bug['UI_Join']==ui].reset_index(drop=True)\n",
        "        cumsum_error = df_cr_ui_error['ttl_bug_inc'].shift().cumsum().fillna(0)\n",
        "        cumsum_error_rate = cumsum_error / (cumsum_error.index+1)\n",
        "        df_cr_ui_error['cumsum_error_rate'] = cumsum_error_rate\n",
        "        cr_ui_error_rate.update(df_cr_ui_error.set_index(['CR', 'UI_Join'])['cumsum_error_rate'].to_dict())\n",
        "\n",
        "    ### CR level module error rate\n",
        "    cr_error_rate_uptodate = {}\n",
        "    for cr_id, cr_group in df_task.groupby('CR'):\n",
        "        ui_effort = cr_group.groupby('UI_Join')['Est. Effort'].sum()\n",
        "        ui_err_rate = np.average(a=[cr_ui_error_rate.get((cr_id, ui), DEFAULT_BUG_RATE) for ui in ui_effort.index],\n",
        "                                    weights=ui_effort.values)\n",
        "        cr_error_rate_uptodate[cr_id] = ui_err_rate\n",
        "\n",
        "    df['UI error rate'] = df['Ticket'].apply(lambda x: cr_error_rate_uptodate[x])\n",
        "\n",
        "    return df, df_task"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695115870935
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "\n",
        "    # credit_df = pd.read_csv(args.data, header=1, index_col=0)\n",
        "    df, df_task, df_task_base, df_member, base_cr_id = get_dataframes(args.data, args.train_test_ratio)\n",
        "    df, df_task, module_error_rate, cr_error_rate_uptodate = add_ticket_features(df, df_task, df_task_base)\n",
        "    df, df_task = add_dev_features(df, df_member, df_task)\n",
        "    df = add_other_features(df, base_cr_id)\n",
        "    df, df_task = add_ui_features(df, df_task)\n",
        "\n",
        "    mlflow.log_metric(\"num_samples\", df.shape[0])\n",
        "    mlflow.log_metric(\"num_features\", len(FEATURE_COLS))\n",
        "\n",
        "    x_train = df[df['Ticket'].isin(base_cr_id)][FEATURE_COLS].values\n",
        "    y_train = df[df['Ticket'].isin(base_cr_id)][LABEL_COL]\n",
        "    x_test = df[~df['Ticket'].isin(base_cr_id)][FEATURE_COLS].values\n",
        "    y_test = df[~df['Ticket'].isin(base_cr_id)][LABEL_COL]\n",
        "\n",
        "    sc = StandardScaler(with_mean=False)\n",
        "\n",
        "    x_train = sc.fit_transform(x_train)\n",
        "    x_test = sc.transform(x_test)\n",
        "    joblib.dump(sc, 'std_scaler.bin', compress=True)\n",
        "\n",
        "    # Oversampling\n",
        "    cc = RandomOverSampler(random_state=RANDOM_STATE)\n",
        "    over_x_train, over_y_train = cc.fit_resample(x_train, y_train)\n",
        "\n",
        "    train_df = pd.DataFrame(over_x_train, columns = FEATURE_COLS)\n",
        "    train_df['is_bug_inc'] = over_y_train.values\n",
        "    train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "    test_df = pd.DataFrame(x_test, columns = FEATURE_COLS)\n",
        "    test_df['is_bug_inc'] = y_test.values\n",
        "    test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "\n",
        "    mlflow.log_metric('train size', train_df.shape[0])\n",
        "    mlflow.log_metric('test size', test_df.shape[0])"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695115871304
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyArgs:\n",
        "    def __init__(self, /, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "args = MyArgs(\n",
        "            data = \"../../CRRisk/data/Dataset_Deployment_Incident_prediction.xlsx\",\n",
        "            train_test_ratio = 0.75,\n",
        "            train_data = \"local_run/prep_outputs/train\",\n",
        "            test_data = \"local_run/prep_outputs/test\",\n",
        "            )\n",
        "\n",
        "os.makedirs(args.train_data, exist_ok = True)\n",
        "os.makedirs(args.test_data, exist_ok = True)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695115872686
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.start_run()\n",
        "\n",
        "lines = [\n",
        "    f\"Raw data path: {args.data}\",\n",
        "    f\"Train test ratio: {args.train_test_ratio}\",\n",
        "    f\"Train dataset output path: {args.train_data}\",\n",
        "    f\"Test dataset path: {args.test_data}\",\n",
        "\n",
        "]\n",
        "\n",
        "for line in lines:\n",
        "    print(line)\n",
        "\n",
        "main(args)\n",
        "\n",
        "mlflow.end_run()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Raw data path: ../../CRRisk/data/Dataset_Deployment_Incident_prediction.xlsx\nTrain test ratio: 0.75\nTrain dataset output path: local_run/prep_outputs/train\nTest dataset path: local_run/prep_outputs/test\ndata=../../CRRisk/data/Dataset_Deployment_Incident_prediction.xlsx train_test_ratio=0.75 train_data=local_run/prep_outputs/train test_data=local_run/prep_outputs/test\ninput data: ../../CRRisk/data/Dataset_Deployment_Incident_prediction.xlsx\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695115905986
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls \"local_run/prep_outputs/train\" "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[0m\u001b[01;32mdata.csv\u001b[0m*\r\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695115944793
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "crrisk",
      "language": "python",
      "display_name": "CR Risk Env"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.17",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "crrisk"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}